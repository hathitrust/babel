#!/l/local/bin/perl

=head1 NAME

index-j

=head1 USAGE

% index-j -r run

=head1 DESCRIPTION

Take a slice of available items from the indexing queue, build docs for
the ids and send to Solr for indexing.  Record error ids in j_errors
table.  Record indexed status in j_indexed table.

Select the shard into which the item will be indexed by round-robin
scheduling.  If an item reappears later in the queue (rights changed,
etc.), consult the j_indexed table to select the correct shard for
re-indexing so the item does not get indexed in more than one shard.

The run number is associated with various experiments related to Solr
configuration and is used to compose the name of the config file. So
the config file for run number 1 would be
$SDRROOT/path-to-config-files/run-1.conf The config file points a solr
instance for each available shard.

Commit and optimize are manual processes or handled as part of the
driver_j process flow.

Protect shard from exceeding max shard size.  Co-operates with
sizer-j.  sizer-j will disable shard and re-enable shard to control
the flow of documents into a given shard.

=head1 OPTIONS

=over 8

=item -

see help

=back

=cut

use strict;

# ----------------------------------------------------------------------
# Set up paths for local libraries -- must come first
# ----------------------------------------------------------------------
use lib "$ENV{SDRROOT}/mdp-lib/Utils";
use Vendors;


# Perl
use CGI;
use Getopt::Std;
use Time::HiRes;

# App
use Utils;
use Utils::Time;
use Debug::DUtils;

use Context;
use Identifier;
use MdpConfig;
use Database;
use Search::Indexer;
use Search::Constants;
use ObjFactory;
use Utils::GlobalSwitch;

# Local
use Db;
use SLIP_Utils::Db_driver;
use SLIP_Utils::Common;
use SLIP_Utils::IndexerPool;
use SLIP_Utils::Solr;
use SLIP_Utils::States;
use SLIP_Utils::Log;

Utils::GlobalSwitch::Exit_If_cron_jobs_disabled('STOPSLIP');

# ---------------------------------------------------------------------

=item i_get_usage

Description

=cut

# ---------------------------------------------------------------------
sub i_get_usage {
    my $s .= qq{Usage: index-j -r run [-H host][-I id][-d 1|2|3][-v]
            where -I id indexes one id
                  -H hostname overrides \`hostname\` call
                  -v turns on verbose logging
                  -d sets the debug level
                      where 1=default
                            2=add vufind response
                            3=add docfulldebug (no delete of zip tmpdir)\n};
    return $s;
}

# When total docs reaches an even multiple of the stop point the
# indexer stops.

our ($opt_d, $opt_r, $opt_I, $opt_H, $opt_v);

my $ops = getopts('d:r:I:vH:');

my $VERBOSE_LOGGING = defined($opt_v);

if (defined($opt_d)) {
    if (! grep(/1|2|3/, ($opt_d))) {
        my $s = i_get_usage();
        __output($s);
        exit $SLIP_Utils::States::RC_BAD_ARGS;
    }

    $ENV{'DEBUG'} = 'lsdb,idx,doc,me';
    $ENV{'DEBUG'} .= ',vufind'
        if ($opt_d == 2);
    $ENV{'DEBUG'} .= ',docfulldebug'
        if ($opt_d == 3);

    $VERBOSE_LOGGING = 1;
}

# Required
my $RUN = $opt_r;
my $PID = $$;
my $HOST = $opt_H;
if (! defined($HOST)) {
    $HOST = `hostname`; chomp($HOST); $HOST =~ s,\..*$,,;
}

if (! $opt_r) {
    my $s = i_get_usage();
    __output($s);
    exit $SLIP_Utils::States::RC_BAD_ARGS;
}

# Flush i/o
$| = 1;

my $C = new Context;

my $global_configfile = $ENV{'SDRROOT'} . qq{/slip/Config/run-$RUN.conf};
my $common_configfile = $ENV{'SDRROOT'} . qq{/slip/Config/common.conf};

my $config = new MdpConfig($common_configfile, $global_configfile);
$C->set_object('MdpConfig', $config);

my $db;
eval {
    $db = new Database($config);
};
if ($@) {
    SLIP_Utils::Common::Log_database_connection_error($C, 'index-j', $@);
    exit $SLIP_Utils::States::RC_DATABASE_CONNECT;
}

$C->set_object('Database', $db);

my $DBH = $db->get_DBH();

my $DEFAULT_SLICE_SIZE = $config->get('queue_slice_size');

# Index just one id
my $SINGLE_ID = $opt_I;

my $GLOBAL_ref_to_ary_of_id_hashref = [];
my $GLOBAL_ref_to_ary_of_id_error_recovery_hashref = [];

# Initialize a pool of Indexers with HTTP timeout=30 sec (default)
my $INDEXER_POOL = new SLIP_Utils::IndexerPool($C, $DBH, $RUN);
my $GLOBAL_SHARD_FOR_ERROR_REPORT = 0;

my $PROCESSING = 1;
my $MAX_ERRORS_SEEN = 0;
my $INTERACTIVE = $ENV{'TERM'};

eval {
    i_processing_loop($C);
};
if ($@) {
    # Place remaining ids on the error list, disable the shard that had
    # the error and log it.
    handle_critical_error($C, $DBH, $RUN, $GLOBAL_SHARD_FOR_ERROR_REPORT, $PID, $HOST, $@);
    # NOTREACHED
}

exit $MAX_ERRORS_SEEN;

#
# ---------------------  Main  S u b r o u t i n e s   -------------------------
#


# ---------------------------------------------------------------------

=item i_processing_loop

Description

=cut

# ---------------------------------------------------------------------
sub i_processing_loop {
    my $C = shift;

    my $item_ct = 0;
    my $slices_taken = 0;

    while ($PROCESSING) {
        # Exit this producer if the stop file is touched
        Utils::GlobalSwitch::Exit_If_cron_jobs_disabled('STOPSLIP');
        # POSSIBLY NOTREACHED

        # Exit this producer if time of day falls between the downtime
        # for VuFind Solr. We need VuFind for Solr document metadata.
        handle_vSolr_downtime($C);
        # POSSIBLY NOTREACHED

        # Exit this producer before (another) slice is taken and
        # marked as available for indexing if max configured producers
        # are running
        handle_max_producers_running($C, $DBH, $RUN, $HOST);
        # POSSIBLY NOTREACHED

        # Exit before (another) slice is taken if this host is
        # disabled
        handle_host_enabled($C, $DBH, $RUN, $PID, $HOST, $slices_taken);
        # POSSIBLY NOTREACHED

        # Exit if no shards are enabled to receive data
        handle_indexer_pool($C, $RUN, $PID, $HOST, $slices_taken);
        # POSSIBLY NOTREACHED

        # OK to proceed: get a slice of ids from the queue or just one
        # id and save in the GLOBAL error recovery id array
        my $num_ids = Read_indexing_queue($C);

        # Exit if the queue is empty
        handle_queue_empty($C, $RUN, $PID, $HOST, $num_ids);
        # POSSIBLY NOTREACHED

        $slices_taken++;

        # Logging
        handle_startup($C, $DBH, $RUN, $PID, $HOST, $slices_taken, $num_ids);

        # Go to work
        while (my $hashref = get_id_from_slice()) {
            my $start = Time::HiRes::time();

            my $id = $hashref->{'id'};

            # Get the correct indexer for this id.  If this is a
            # re-index and the shard it belongs in is suspended, the
            # id will be added to the error list.
            my ($index_state, $ocr_status, $metadata_status, $stats_ref);
            my ($indexer, $shard, $random) = get_Next_indexer($C, $DBH, $RUN, $PID, $HOST, $id);
            if ($indexer) {
                # Index
                ($index_state, $ocr_status, $metadata_status, $stats_ref) =
                    process_one_id($C, $DBH, $RUN, $id, $indexer);
            }
            else {
                ($index_state, $ocr_status, $metadata_status) =
                    (IX_NO_INDEXER_AVAIL, IX_NO_ERROR, IX_NO_ERROR);
            }

            $item_ct++;

            my $item_is_Solr_indexed =
                handle_i_result($C, $DBH, $RUN, $shard, $id, $PID, $HOST,
                                $index_state, $ocr_status, $metadata_status);

            my $reindexed = 0;
            if ($item_is_Solr_indexed) {
                $reindexed = update_ids_indexed($C, $DBH, $RUN, $shard, $id);
            }

            Log_item($C, $RUN, $shard, $id, $PID, $HOST, $stats_ref, $item_ct,
                     $index_state, $ocr_status, $metadata_status, $random, $reindexed);

            # Item is now recorded in either mdp.j_errors or
            # mdp.j_indexed or in mdp.j_indexed AND mdp.j_timeouts.
            my $shard_num_docs_processed =
                update_stats($C, $DBH, $RUN, $shard, $stats_ref, $start, $index_state);

            update_checkpoint($C, $DBH, $RUN, $shard, time(), $shard_num_docs_processed);

            remove_id_from_backup_list();

            # Remove item from the input queue. Critical errors (9)
            # cause a longjump out of i_processing_loop() and add IDs
            # to j_errors and deletes them from j_queue atomically
            # without resorting to a call to handle_dequeue().
            handle_dequeue($C, $DBH, $RUN, $id, $PID, $HOST);

            handle_timeout_delay($C, $DBH, $RUN, $PID, $shard, $HOST, $index_state, $item_ct, $id);
        }
    }
}

#
# --------------- Helper  S u b r o u t i n e s   @@-------------------
#

# ---------------------------------------------------------------------

=item get_id_from_slice

Description

=cut

# ---------------------------------------------------------------------
sub get_id_from_slice {
    my $item_hashref = shift @$GLOBAL_ref_to_ary_of_id_hashref;
    return $item_hashref;
}

# ---------------------------------------------------------------------

=item remove_id_from_backup_list

Description

=cut

# ---------------------------------------------------------------------
sub remove_id_from_backup_list {
    my $id = shift @$GLOBAL_ref_to_ary_of_id_error_recovery_hashref;
}


# ---------------------------------------------------------------------

=item process_one_id

Description

=cut

# ---------------------------------------------------------------------
sub process_one_id {
    my ($C, $dbh, $run, $id, $indexer) = @_;

    my %merged_stats;

    # --------------------  Create Document  --------------------
    my $of = new ObjFactory;
    my $document_subclass = $C->get_object('MdpConfig')->get('document_subclass');
    my %of_attrs = (
                    'class_name' => $document_subclass,
                    'parameters' => {
                                     'C'   => $C,
                                     'dbh' => $dbh,
                                     'id'  => $id,
                                    },
                   );
    my $doc = $of->create_instance($C, \%of_attrs);
    my ($ocr_status, $metadata_status) = $doc->get_document_status();

    my $doc_build_failure = ($ocr_status != IX_NO_ERROR) || ($metadata_status != IX_NO_ERROR);
    my $index_state = IX_NO_ERROR;

    # --------------------  Index Document  --------------------
    if (! $doc_build_failure) {
        my $idx_stats_ref;
        ($index_state, $idx_stats_ref) = $indexer->index_document($C, $doc);

        my $doc_stats_ref = $doc->get_document_stats($C);
        %merged_stats = %$doc_stats_ref;
        %merged_stats = (%merged_stats, %$idx_stats_ref);
    }

    return ($index_state, $ocr_status, $metadata_status, \%merged_stats);
}



# ---------------------------------------------------------------------

=item get_Next_indexer

If id already indexed, select an indexer that will re-index it in into
the correct shard otherwise just take whichever indexer comes up next.

If all indexers in the pool or the specific indexer for the shard
are/is waiting, this call will block until the wait has expired for at
least one indexer in the pool or the specific indexer for the shard,
as the case may be.

=cut

# ---------------------------------------------------------------------
sub get_Next_indexer {
    my ($C, $dbh, $run, $pid, $host, $id) = @_;

    my ($indexer, $shard);

    my $s = '';
    my $shard_of_id = Db::Select_item_id_shard($C, $dbh, $run, $id);
    my $random = (! $shard_of_id);

    if ($shard_of_id) {
        ($indexer, $shard) = $INDEXER_POOL->get_indexer_For_shard($C, $shard_of_id);
    }
    else {
        ($indexer, $shard) = $INDEXER_POOL->get_indexer($C);
    }

    $GLOBAL_SHARD_FOR_ERROR_REPORT = $shard;

    return ($indexer, $shard, $random);
}


# ---------------------------------------------------------------------

=item Read_indexing_queue

-I ($SINGLE_ID) NOT intended to run from a cron job

=cut

# ---------------------------------------------------------------------
sub Read_indexing_queue {
    my $C = shift;

    if ($SINGLE_ID) {
        my %item_hash = (
                         'id' => $SINGLE_ID,
                        );
        $GLOBAL_ref_to_ary_of_id_hashref = [\%item_hash];
        $PROCESSING = 0;
    }
    else {
        $GLOBAL_ref_to_ary_of_id_hashref =
            Db::Select_id_slice_from_queue($C, $DBH, $RUN, $PID, $HOST, $DEFAULT_SLICE_SIZE);

        # Put contents of @$GLOBAL_ref_to_ary_of_id_hashref back into
        # queue if critical error or lack of indexers leaves sliced
        # items unprocessed.
        push(@$GLOBAL_ref_to_ary_of_id_error_recovery_hashref,
             @$GLOBAL_ref_to_ary_of_id_hashref);
    }

    return scalar(@$GLOBAL_ref_to_ary_of_id_hashref);
}



# ---------------------------------------------------------------------

=item max_errors_reached

Description

=cut

# ---------------------------------------------------------------------
sub max_errors_reached {
    my ($C, $dbh, $run, $shard) = @_;

    my $config = $C->get_object('MdpConfig');

    # Solr could not parse doc
    my $max_I = $config->get('max_indx_errors');
    # Could not create OCR for Solr doc
    my $max_O = $config->get('max_ocr__errors');
    # Could not get metadata for Solr doc
    my $max_M = $config->get('max_meta_errors');
    # Server unavailable
    my $max_S = $config->get('max_serv_errors');
    # Serious stuff
    my $max_C = $config->get('max_crit_errors');
    my $max_N = $config->get('max_no_indexer_avail');

    my ($num_errors, $num_I, $num_O, $num_M, $num_C, $num_S, $num_N) =
        Db::Select_error_data($C, $dbh, $run, $shard);

    my ($condition, $num, $max);

    my $max_I_seen = ($num_I > $max_I);
    if ($max_I_seen) {
        $condition = 'I'; $num = $num_I; $max = $max_I;
    }
    my $max_O_seen = ($num_O > $max_O);
    if ($max_O_seen) {
        $condition = 'O'; $num = $num_O; $max = $max_O;
    }
    my $max_M_seen = ($num_M > $max_M);
    if ($max_M_seen) {
        $condition = 'M'; $num = $num_M; $max = $max_M;
    }
    my $max_C_seen = ($num_C > $max_C);
    if ($max_C_seen) {
        $condition = 'C'; $num = $num_C; $max = $max_C;
    }
    my $max_S_seen = ($num_S > $max_S);
    if ($max_S_seen) {
        $condition = 'S'; $num = $num_S; $max = $max_S;
    }
    my $max_N_seen = ($num_N > $max_N);
    if ($max_N_seen) {
        $condition = 'N'; $num = $num_N; $max = $max_N;
    }

    my $max_errors_seen =
        (
         $max_I_seen
         ||
         $max_O_seen
         ||
         $max_M_seen
         ||
         $max_S_seen
         ||
         $max_C_seen
         ||
         $max_N_seen
        );

    return ($max_errors_seen, $condition, $num, $max);
}

# ---------------------------------------------------------------------

=item update_ids_indexed

Description

=cut

# ---------------------------------------------------------------------
sub update_ids_indexed {
    my ($C, $dbh, $run, $shard, $id) = @_;

    return Db::insert_item_id_indexed($C, $dbh, $run, $shard, $id);
}


# ---------------------------------------------------------------------

=item update_stats

each shard producer updates its row by adding its stats for EACH ITEM
processed - may be more than one producer per shard

=cut

# ---------------------------------------------------------------------
sub update_stats {
    my ($C, $dbh, $run, $shard, $stats_ref, $start) = @_;

    my $tot_Time = Time::HiRes::time() - $start;

    my $doc_size = $$stats_ref{'update'}{'doc_size'};
    my $doc_Time = $$stats_ref{'create'}{'elapsed'};
    my $idx_Time = $$stats_ref{'update'}{'elapsed'};

    my ($shard_num_docs_processed) =
        Db::update_shard_stats($C, $dbh, $run, $shard, $doc_size, $doc_Time, $idx_Time, $tot_Time);

    my $t = sprintf(qq{sec=%.2f}, $tot_Time);
    DEBUG('doc,idx', qq{TOTAL: processed in $t});

    return $shard_num_docs_processed;
}

# ---------------------------------------------------------------------

=item update_checkpoint

Description. try every 10 instead of 100 for finer granularity

=cut

# ---------------------------------------------------------------------
sub update_checkpoint {
    my ($C, $dbh, $run, $shard, $now, $shard_num_docs_processed) = @_;

    if (($shard_num_docs_processed % 100) == 0) {
        Db::update_rate_stats($C, $dbh, $run, $shard, $now);
    }
}


#
# --------------- Handling  S u b r o u t i n e s   @@-------------------
#

# ---------------------------------------------------------------------

=item handle_vSolr_downtime

Description

=cut

# ---------------------------------------------------------------------
sub handle_vSolr_downtime {
    my $C = shift;
    my $config = $C->get_object('MdpConfig');

    return
        if (! $config->get('vSolr_downtime_checking'));

    my @downtime = $config->get('vSolr_downtime_interval');

    my ($second, $minute, $hour) = localtime();
    my $decimal_hour = $hour + $minute/60.0;

    if (
        ($decimal_hour > $downtime[0])
        &&
        ($decimal_hour < $downtime[1])
       ) {
        exit 0;
    }
}


# ---------------------------------------------------------------------

=item handle_timeout_delay

We saw JVM thread exhaustion when we continued to submit doc updates
while Solr was under a heavy load.  This code may prevent that.

By setting a wait for the indexer in the pool we prevent it from being
returned when we ask for an indexer until the wait time has elapsed.

=cut

# ---------------------------------------------------------------------
sub handle_timeout_delay {
    my ($C, $dbh, $run, $pid, $shard, $host, $index_state, $ct, $id, $was_indexed) = @_;

    if ($index_state == IX_INDEX_TIMEOUT) {
        my $Wait_For_secs = $INDEXER_POOL->set_shard_waiting($C, $dbh, $run, $shard);
        Log_timeout($C, $run, $pid, $shard, $host, $Wait_For_secs, $ct, $id);
    }
    else {
        my $was_waiting = $INDEXER_POOL->Reset_shard_waiting($C, $shard);
        Log_timeout($C, $run, $pid, $shard, $host, 0, $ct, 'noop')
            if ($was_waiting);
    }
}


# ---------------------------------------------------------------------

=item handle_max_producers_running

Description

=cut

# ---------------------------------------------------------------------
sub handle_max_producers_running {
    my ($C, $dbh, $run, $host) = @_;

    my ($max_running, $num_configured, $num_running) =
        SLIP_Utils::Common::max_producers_running($C, $dbh, $run, $host);

    if ($max_running && ($num_running-1 > 0)) {
        Log_max_producers_seen($C, $run, $PID, $host, $num_configured, $num_running-1);
        DEBUG('me', qq{DEBUG: max producers running ... exit});
        exit 0;
    }
}

# ---------------------------------------------------------------------

=item handle_startup

Description

=cut

# ---------------------------------------------------------------------
sub handle_startup {
    my ($C, $dbh, $run, $pid, $host, $slices_taken, $num_ids) = @_;

    # Log first time through slice-taking loop
    if ($slices_taken == 1) {
        Log_startup($C, $run, $pid, $host);
        DEBUG('me', qq{DEBUG: startup});
    }

    Log_slice($C, $run, $pid, $host, $num_ids);
    DEBUG('me', qq{DEBUG: process slice num=$num_ids});
}


# ---------------------------------------------------------------------

=item handle_host_enabled

Description

=cut

# ---------------------------------------------------------------------
sub handle_host_enabled {
    my ($C, $dbh, $run, $pid, $host, $work_done) = @_;

    my $host_enabled = Db::Select_host_enabled($C, $dbh, $run, $host);

    if (! $host_enabled) {
        Log_host_Shutdown($C, $run, $pid, $host)
            if ($work_done);
        DEBUG('me', qq{DEBUG: host=$host stop requested, slices taken=$work_done ... exit});
        exit 0;
    }
}


# ---------------------------------------------------------------------

=item handle_indexer_pool

Description

=cut

# ---------------------------------------------------------------------
sub handle_indexer_pool {
    my ($C, $run, $pid, $host, $work_done) = @_;

    my $num_indexers_available = $INDEXER_POOL->get_num_indexers_available($C);

    if ($num_indexers_available == 0)
    {
        Log_shard_Shutdown($C, $run, $pid, $host)
            if ($work_done);
        DEBUG('me', qq{DEBUG: no unsuspended shards enabled, host=$host slices taken=$work_done ... exit});
        exit 0;
    }
}

# ---------------------------------------------------------------------

=item handle_dequeue

Description

=cut

# ---------------------------------------------------------------------
sub handle_dequeue {
    my ($C, $dbh, $run, $id, $pid, $host) = @_;

    Db::dequeue($C, $dbh, $run, $id, $pid, $host);
    Log_dequeue($C, $run, $id, $pid, $host);
}

# ---------------------------------------------------------------------

=item handle_queue_empty

Description

=cut

# ---------------------------------------------------------------------
sub handle_queue_empty {
    my ($C, $run, $pid, $host, $num_sliced_from_queue) = @_;

    if ($num_sliced_from_queue == 0) {
        Log_queue_empty($C, $run, $pid, $host);
        DEBUG('me', qq{DEBUG: queue empty ... exit});
        exit 0;
    }
}

# ---------------------------------------------------------------------

=item handle_i_result

All errors (indexing, ocr, metadata) are put in the error list and not
counted as indexed in j_indexed.

Timesouts

=cut

# ---------------------------------------------------------------------
sub handle_i_result {
    my ($C, $dbh, $run, $shard, $id, $pid, $host, $index_state, $ocr_status, $metadata_status) = @_;

    # Optimistic
    my $item_is_Solr_indexed = 1;

    # determine reason code in priority order: 1)indexing, 2)ocr, 3)metadata.
    my $index_ok = (! Search::Constants::indexing_failed($index_state));
    my $ocr_ok = ($ocr_status == IX_NO_ERROR);
    my $metadata_ok = ($metadata_status == IX_NO_ERROR);

    my $reason;
    if (! $index_ok) {
        $reason = $index_state;
        $item_is_Solr_indexed = 0;
    }
    elsif (! $ocr_ok) {
        $reason = $ocr_status;
        $item_is_Solr_indexed = 0;
    }
    elsif (! $metadata_ok) {
        $reason = $metadata_status;
        $item_is_Solr_indexed = 0;
    }

    # IX_INDEX_TIMEOUT is NOT an indexing error and thus the id will
    # be recorded in the j_indexed table (downstream).
    # IX_INDEX_TIMEOUT is (probably) an HTTP timeout and the server
    # will complete the request. Still, to be sure the server
    # completed the request, record the id in the timeout table so
    # that when timeouts are reprocessed the request can be re-tried
    # and with the correct shard (now recorded in j_indexed).
    if ($index_state == IX_INDEX_TIMEOUT) {
        Db::insert_item_id_timeout($C, $dbh, $run, $id, $shard, $pid, $host);
    }

    if (! $item_is_Solr_indexed) {
        Db::insert_item_id_error($C, $dbh, $run, $shard, $id, $pid, $host, $reason);

        my ($max_errors_seen, $condition, $num, $max) = max_errors_reached($C, $dbh, $run, $shard);
        if ($max_errors_seen && (! $MAX_ERRORS_SEEN)) {
            $MAX_ERRORS_SEEN = $SLIP_Utils::States::RC_MAX_ERRORS;

            Log_error_stop($C, $run, $shard, $pid, $host, 'MAX ERRORS condition=$condition num=$num)');

            my $subj = qq{[SLIP] MAX ERRORS: run=$run shard=$shard disabled};
            my $msg =
                qq{ERROR point reached for run=$run shard=$shard pid=$pid host=$host\n} .
                    qq{condition=$condition num=$num  max=$max};
            # One email, not one for every unprocessed slice item
            SLIP_Utils::Common::Send_email($C, 'report', $subj, $msg);

            Db::update_shard_enabled($C, $dbh, $run, $shard, 0);
            Db::set_shard_build_error($C, $dbh, $run, $shard);
        }
    }

    return $item_is_Solr_indexed;
}


# ---------------------------------------------------------------------

=item handle_critical_error

Description

=cut

# ---------------------------------------------------------------------
sub handle_critical_error {
    my ($C, $dbh, $run, $shard, $pid, $host, $error) = @_;

    # Have we lost the database?
    my $connection_still_alive = $dbh->ping();
    if ($connection_still_alive) {
        Db::set_shard_build_error($C, $dbh, $run, $shard)
                if (! $INTERACTIVE);

        # Un-handled ids are in the GLOBAL error array. Put them in
        # the error list and disable the shard.
        while (my $item_hashref = shift @$GLOBAL_ref_to_ary_of_id_error_recovery_hashref) {
            my $id = $item_hashref->{'id'};
            Db::insert_item_id_error($C, $dbh, $run, $shard, $id, $pid, $host, IX_CRITICAL_FAILURE);
        }
        # Turn off this shard.
        Db::update_shard_enabled($C, $dbh, $run, $shard, 0);
    }
    else {
        # No database to talk to.  Save the ids in an error log file
        # and restore them to j_queue, later, by hand.
        while (my $item_hashref = shift @$GLOBAL_ref_to_ary_of_id_error_recovery_hashref) {
            my $id = $item_hashref->{'id'};
            Log_critical($C, $run, $id);
        }
    }

    my $subj = qq{[SLIP] CRITICAL ERROR: run=$run shard=$shard};
    my $msg =
        qq{CRITICAL ERROR } .
            qq{run=$run shard=$shard pid=$pid host=$host\nerror=$error\n};
    $msg .= qq{database connection lost\nunprocessed restorable ids logged in file=logs/___DATE___/run-$run/critical_ids___PID___.log\n}
        if (! $connection_still_alive);
    SLIP_Utils::Common::Send_email($C, 'report', $subj, $msg);

    Log_error_stop($C, $run, $shard, $pid, $host, qq{CRITICAL ERROR: $error});

    my $rc = $SLIP_Utils::States::RC_CRITICAL_ERROR;

    __output($msg);
    __non_interactive_err_output($rc, $msg);

    exit $rc;
}



#
# --------------------- Logging  S u b r o u t i n e s   @@-------------------------
#

# ---------------------------------------------------------------------

=item Log_critical

Description

=cut

# ---------------------------------------------------------------------
sub Log_critical {
    my ($C, $run, $id) = @_;

    my $s = Utils::Time::iso_Time() . qq{ id=$id};
    SLIP_Utils::Log::this_string($C, $s, 'critical_ids_logfile', '___RUN___', $run, 1);
}

# ---------------------------------------------------------------------

=item Log_dequeue

Description

=cut

# ---------------------------------------------------------------------
sub Log_dequeue {
    my ($C, $run, $id, $pid, $host) = @_;

    my $s = qq{***DEQUEUE: } . Utils::Time::iso_Time() . qq{ id=$id r=$run pid=$pid h=$host};
    SLIP_Utils::Log::this_string($C, $s, 'indexer_logfile', '___RUN___', $run, 1)
            if ($VERBOSE_LOGGING);
}

# ---------------------------------------------------------------------

=item Log_slice

Description; Procdeural interface

=cut

# ---------------------------------------------------------------------
sub Log_slice {
    my ($C, $run, $pid, $host, $num_sliced_from_queue) = @_;

    my $s = qq{***SLICE: } . Utils::Time::iso_Time() . qq{ r=$run pid=$pid h=$host slice_size=$num_sliced_from_queue};
    SLIP_Utils::Log::this_string($C, $s, 'indexer_logfile', '___RUN___', $run, 1)
            if ($VERBOSE_LOGGING);
}

# ---------------------------------------------------------------------

=item Log_queue_empty

Description; Procdeural interface

=cut

# ---------------------------------------------------------------------
sub Log_queue_empty {
    my ($C, $run, $pid, $host) = @_;

    my $s = qq{***QUEUE EMPTY (exit): } . Utils::Time::iso_Time() . qq{ r=$run pid=$pid h=$host};
    SLIP_Utils::Log::this_string($C, $s, 'indexer_logfile', '___RUN___', $run, 1)
            if ($VERBOSE_LOGGING);
}

# ---------------------------------------------------------------------

=item Log_startup

Description; Procdeural interface

=cut

# ---------------------------------------------------------------------
sub Log_startup {
    my ($C, $run, $pid, $host) = @_;

    my $s = qq{***START: } . Utils::Time::iso_Time() . qq{ r=$run pid=$pid h=$host};
    SLIP_Utils::Log::this_string($C, $s, 'indexer_logfile', '___RUN___', $run, 1);
}


# ---------------------------------------------------------------------

=item Log_host_Shutdown

Description

=cut

# ---------------------------------------------------------------------
sub Log_host_Shutdown {
    my ($C, $run, $pid, $host) = @_;

    my $s = qq{***STOP (host): } . Utils::Time::iso_Time() . qq{ r=$run pid=$pid h=$host};
    SLIP_Utils::Log::this_string($C, $s, 'indexer_logfile', '___RUN___', $run, 1);
}

# ---------------------------------------------------------------------

=item Log_shard_Shutdown

Description

=cut

# ---------------------------------------------------------------------
sub Log_shard_Shutdown {
    my ($C, $run, $pid, $host) = @_;

    my $s = qq{***STOP (no shards): } . Utils::Time::iso_Time() . qq{ r=$run pid=$pid h=$host};
    SLIP_Utils::Log::this_string($C, $s, 'indexer_logfile', '___RUN___', $run, 1);
}

# ---------------------------------------------------------------------

=item Log_max_producers_seen

Description

=cut

# ---------------------------------------------------------------------
sub Log_max_producers_seen {
    my ($C, $run, $pid, $host, $num_configured, $num_running) = @_;

    my $s = qq{***MAX PRODUCERS (exit): } . Utils::Time::iso_Time() . qq{ r=$run pid=$pid h=$host num_cfgd=$num_configured num_running=$num_running};
    SLIP_Utils::Log::this_string($C, $s, 'indexer_logfile', '___RUN___', $run, 1)
            if ($VERBOSE_LOGGING);
}


# ---------------------------------------------------------------------

=item Log_item

Description

=cut

# ---------------------------------------------------------------------
sub Log_item {
    my ($C, $run, $shard, $id, $pid, $host, $stats_ref, $ct, $index_state, $ocr_status, $metadata_status, $random, $reindexed) = @_;

    my $buf;

    # DOC d_t = sec, d_kb = Kb, d_kbs = Kb/sec
    my $d_t = $$stats_ref{'create'}{'elapsed'};
    my $d_k = $$stats_ref{'create'}{'doc_size'}/1024;
    my $d_kbs = ($d_t > 0) ? $d_k/$d_t : 0;

    $buf .= sprintf(qq{ d_k=%.1f d_t=%.2f d_kbs=%.2f}, $d_k, $d_t, $d_kbs);

    # IDX i_t = sec i_mbs = Mb/sec
    my $i_t = $$stats_ref{'update'}{'elapsed'};
    my $i_mbs = ($i_t > 0) ? $$stats_ref{'update'}{'doc_size'}/1024/1024/$i_t : 0;

    $buf .= sprintf(qq{ i_t=%.2f i_mbs=%.2f}, $i_t, $i_mbs);

    my $error = '';
    $error .= ' - ' . SLIP_Utils::Common::IXconstant2string($index_state)
        if (Search::Constants::indexing_failed($index_state));
    $error .= ' - ' . SLIP_Utils::Common::IXconstant2string($ocr_status)
        if ($ocr_status != IX_NO_ERROR);
    $error .= ' - ' . SLIP_Utils::Common::IXconstant2string($metadata_status)
        if ($metadata_status != IX_NO_ERROR);

    my $ri = '';
    if ($reindexed) {
        $ri = ' - REINDEX';
    }

    $shard = $random ? qq{rand_$shard} : qq{REQD_$shard};
    my $s = qq{ITEM[$ct$ri$error]: } . Utils::Time::iso_Time() . qq{ r=$run s=$shard id=$id pid=$pid h=$host} . $buf;
    SLIP_Utils::Log::this_string($C, $s, 'indexer_logfile', '___RUN___', $run, 1);
}


# ---------------------------------------------------------------------

=item Log_error_stop

Description

=cut

# ---------------------------------------------------------------------
sub Log_error_stop {
    my ($C, $run, $shard, $pid, $host, $s) = @_;

    my $ss = qq{***ERROR STOP: } . Utils::Time::iso_Time() . qq{ run=$run pid=$pid host=$host shard=$shard stop=$s};
    SLIP_Utils::Log::this_string($C, $ss, 'indexer_logfile', '___RUN___', $run, 1);
}


# ---------------------------------------------------------------------

=item Log_timeout

Description

=cut

# ---------------------------------------------------------------------
sub Log_timeout {
    my ($C, $run, $pid, $shard, $host, $delay, $ct, $id) = @_;

    my $delay_str = $delay ? "***TIMEOUT DELAY[$ct]" : "***TIMEOUT RESET[$ct]" ;

    my $s = qq{$delay_str: } . Utils::Time::iso_Time() . qq{ r=$run s=$shard $id pid=$pid host=$host delay=$delay};
    SLIP_Utils::Log::this_string($C, $s, 'indexer_logfile', '___RUN___', $run, 1);
}

1;


=head1 AUTHOR

Phillip Farber, University of Michigan, pfarber@umich.edu

=head1 COPYRIGHT

Copyright 2008-9 Â©, The Regents of The University of Michigan, All Rights Reserved

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject
to the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

=cut
